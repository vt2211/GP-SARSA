# GP-SARSA

Based on Engel et al. (2005), GP-SARSA offers key advantages over traditional Reinforcement Learning methods SARSA and TD by leveraging Gaussian Processes to generalize across state-action spaces without discretization, **particularly excelling in environments with continuous or large state-action spaces**. It provides both Q-value predictions and uncertainty estimates, enhancing exploration through uncertainty-guided strategies. Unlike traditional methods, GP-SARSA handles stochastic environments robustly and supports model-free policy improvement using Bayesian inference. Its kernel-based approach encodes prior knowledge and prevents overfitting by smoothing noisy data. Implementing kernel sparsification ensures computational efficiency by maintaining a compact representation of state-action pairs – instead of storing each (s, a) pair, we maintain a dictionary inclusive only of pairs that are sufficiently dis-similar (w.r.t. the kernel function).

However, GP-SARSA makes assumptions that are **not** trivial for some environments. Regarding uncertainty in the environment – it decomposes uncertainty into two buckets, intrinsic and extrinsic. For intrinsic uncertainty, which arises from the stochastic nature of state transitions and rewards, it assumes that values (i.e. rewards) are corrupted by zero-mean Gaussian noise and that residual errors (differences between predicted and actual values) across different transitions are independent. This assumption ensures that the uncertainty due to randomness in the environment can be modeled as a Gaussian process with tractable posterior updates. For extrinsic uncertainty, which stems from incomplete knowledge of the underlying MDP, it assumes that the Q-function follows a Gaussian process prior, representing the agent's subjective uncertainty about the true value function. This extrinsic uncertainty is captured through a well-defined kernel function that encodes correlations between state-action pairs, reflecting prior beliefs about the problem structure.

The notebook sets up a 2D stochastic maze environment, where the task involves designing a two-dimensional continuous environment within the unit square for a reinforcement learning agent solving a maze navigation problem. The agent takes steps of 0.1 units in any direction (so the action space is the unit circle) and receives a negative reward of -1 at each timestep until it reaches the goal. Hitting an obstacle returns the agent to its original position. 

The stochasticity is divided into two parts – intrinsic and extrinsic uncertainties. The intrinsic uncertainties in the maze environment include reward noise and action noise. Reward noise arises from adding Gaussian perturbations to the base reward, making the observed reward uncertain. Action noise introduces randomness by applying a random rotational disturbance (up to ±36 degrees) to the agent’s intended direction, leading to deviations in movement. Extrinsic uncertainties are in state observation noise, where the agent receives a noisy version of its true position, with Gaussian noise added to the actual state.

It implements a GP-SARSA reinforcement learning agent that uses Gaussian Processes with an evolving dictionary of state-action pairs to learn a Q-value function under uncertainty. The agent explores the maze using epsilon-greedy or variance-based strategies, updates its Q-function based on the temporal difference error, and grows its dictionary for novel experiences. Additionally, the code includes visualization tools to track training progress, plot trajectories, value and variance maps, and eventually display a learned vector field policy over the maze.
